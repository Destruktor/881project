{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_data_file(filename):\n",
    "    Data = pd.read_csv(filename,low_memory = False)\n",
    "\n",
    "    # Drop unneccessary columns #\n",
    "\n",
    "    Data = Data.drop('Vehicle_Annual_Miles', 1)\n",
    "    Data = Data.drop('Vehicle_Comprehensive_Coverage_Limit', 1)\n",
    "    Data = Data.drop('Driver_Minimum_Age', 1)\n",
    "    Data = Data.drop('Driver_Maximum_Age', 1)\n",
    "    Data = Data.drop('EEA_PolicyYear', 1)\n",
    "    Data = Data.drop('Vehicle_New_Cost_Amount', 1)\n",
    "    Data = Data.drop('Vehicle_Make_Description', 1)\n",
    "    Data = Data.drop('EEA_Policy_Zip_Code_3', 1)\n",
    "    # Clearing unneccessary rows #\n",
    "\n",
    "    Data = Data[Data.EEA_Policy_Tenure != -1]\n",
    "    Data = Data[Data.Vehicle_Symbol != -1]\n",
    "    Data = Data[Data.Vehicle_Days_Per_Week_Driven != -1]\n",
    "    Data = Data[Data.Vehicle_Anti_Theft_Device != 'Unknown']\n",
    "\n",
    "    # Replace missing Data #\n",
    "\n",
    "    Data['Policy_Zip_Code_Garaging_Location'] = Data['Policy_Zip_Code_Garaging_Location'].replace('Unknown', '00000')\n",
    "    Data['Vehicle_Miles_To_Work'] = Data['Vehicle_Miles_To_Work'].replace('-1', np.nan)\n",
    "    Data['Vehicle_Passive_Restraint'] = Data['Vehicle_Passive_Restraint'].replace('Unknown', 'Y')\n",
    "    # Data['EEA_Policy_Zip_Code_3'] = Data['EEA_Policy_Zip_Code_3'].replace('Unknown', '000')\n",
    "    Data['Vehicle_Med_Pay_Limit'] = Data['Vehicle_Med_Pay_Limit'].replace('-1', np.nan)\n",
    "    Data['Vehicle_Physical_Damage_Limit'] = Data['Vehicle_Physical_Damage_Limit'].replace('-1', np.nan)\n",
    "    Data['Vehicle_Collision_Coverage_Deductible'] = Data['Vehicle_Collision_Coverage_Deductible'].replace('-1', np.nan)\n",
    "    # Data['EEA_Prior_Bodily_Injury_Limit'] = Data['EEA_Prior_Bodily_Injury_Limit'].replace('-1', np.nan)\n",
    "    # Fill nan data #\n",
    "\n",
    "    Data['Vehicle_Miles_To_Work'].fillna((Data['Vehicle_Miles_To_Work'].mean()), inplace=True)\n",
    "    Data['Vehicle_Med_Pay_Limit'].fillna((Data['Vehicle_Med_Pay_Limit'].mean()), inplace=True)\n",
    "    Data['EEA_Prior_Bodily_Injury_Limit'].fillna('0000', inplace=True)\n",
    "    Data['Vehicle_Bodily_Injury_Limit'].fillna('0000', inplace=True)\n",
    "    Data['Vehicle_Physical_Damage_Limit'].fillna('0000', inplace=True)\n",
    "    Data['Vehicle_Collision_Coverage_Deductible'].fillna('0000', inplace=True)\n",
    "\n",
    "    # move to after splitting up data\n",
    "    # #Categorizing continuous data #\n",
    "\n",
    "    # Data['Vehicle_Miles_To_Work']=pd.cut(Data['Vehicle_Miles_To_Work'], bins=[0, 20,40,60,80,100], include_lowest=True, labels=['lowest', 'low', 'mid', 'high', 'highest'])\n",
    "\n",
    "    # Data['Annual_Premium'] = pd.cut(Data['Annual_Premium'],9, include_lowest=True , labels= ['highest', 'very high', 'moderately high', 'higher', 'medium', 'lower', 'moderately low', 'very low', 'lowest'])\n",
    "\n",
    "    # Save modified data to new csv file #\n",
    "\n",
    "    # Data.to_csv('new_data.csv', sep=',', encoding='utf-8')\n",
    "    return Data\n",
    "\n",
    "Data = read_data_file('training_data_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some training and test data sets\n",
    "\n",
    "# training data for categorical model\n",
    "category_training_data_size = 200000\n",
    "#training data for regression model\n",
    "claims_training_data_size = 200000\n",
    "# testing data\n",
    "testing_data_size = 1000\n",
    "number_test_sets = 100\n",
    "\n",
    "# get data with equal number claims and not\n",
    "category_training_data = Data[:category_training_data_size].copy(deep=True)\n",
    "\n",
    "#Categorizing continuous data #\n",
    "\n",
    "category_training_data['Vehicle_Miles_To_Work']=pd.cut(category_training_data['Vehicle_Miles_To_Work'], bins=[0, 20,40,60,80,100], include_lowest=True, labels=['lowest', 'low', 'mid', 'high', 'highest'])\n",
    "\n",
    "category_training_data['Annual_Premium'] = pd.cut(category_training_data['Annual_Premium'],9, include_lowest=True , labels= ['highest', 'very high', 'moderately high', 'higher', 'medium', 'lower', 'moderately low', 'very low', 'lowest'])\n",
    "\n",
    "\n",
    "claims_training_data = Data[:category_training_data_size].copy(deep=True)#Data[category_training_data_size: category_training_data_size + claims_training_data_size].copy(deep=True)\n",
    "\n",
    "\n",
    "training_data_size = 200000#category_training_data_size + claims_training_data_size\n",
    "test_sets = []\n",
    "for i in range(number_test_sets):\n",
    "    t_set = Data[training_data_size + i * testing_data_size: training_data_size + (i + 1) * testing_data_size].copy(deep=True)\n",
    "    test_sets.append(t_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features:  23\n",
      "Selected Features:  [ True  True  True  True False False False False False False False False\n",
      " False  True False  True False False False False False  True  True  True\n",
      " False  True False False  True  True  True False  True False False False\n",
      " False False False False  True False  True  True  True  True False False\n",
      " False  True  True False  True False  True]\n",
      "Feature Ranking:  [ 1  1  1  1  2 32 19 25 23 30 26 27 18  1 16  1 14 29 21 17 11  1  1  1 24\n",
      "  1 22  3  1  1  1 15  1  9 10  7  6  5  8  4  1 13  1  1  1  1 12 31 28  1\n",
      "  1 20  1 33  1]\n",
      "(200000, 23)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# train model to determine claim or not\n",
    "\n",
    "rfc = RandomForestClassifier(max_features='sqrt')#, max_depth=20)\n",
    "\n",
    "X_t = category_training_data.ix[:, 0:-5]\n",
    "Y_t = category_training_data.ix[:, -4]\n",
    "\n",
    "for col in X_t[1:]:\n",
    "    X_t[col] = X_t[col].astype('category')\n",
    "\n",
    "X_t = X_t.apply(lambda x: x.cat.codes)\n",
    "Y_t = Y_t.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "model = Ridge()\n",
    "rfe = RFE(model, 23)\n",
    "fit = rfe.fit(X_t, Y_t)\n",
    "print(\"Num Features: \", fit.n_features_)\n",
    "print(\"Selected Features: \", fit.support_)\n",
    "print(\"Feature Ranking: \", fit.ranking_)\n",
    "\n",
    "X_t=fit.transform(X_t)\n",
    "\n",
    "print(X_t.shape)\n",
    "rfc.fit(X_t, Y_t)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With claims (7864, 60)\n",
      "without (192136, 60)\n",
      "(7864, 60)\n"
     ]
    }
   ],
   "source": [
    "# get scaled data sets\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# pca = PCA(n_components=.95)\n",
    "\n",
    "categorical_columns = ['Policy_Company', 'Policy_Installment_Term', 'Policy_Billing_Code', 'Policy_Method_Of_Payment', 'Policy_Reinstatement_Fee_Indicator', 'Policy_Zip_Code_Garaging_Location', 'Vehicle_Performance', 'Vehicle_Number_Of_Drivers_Assigned', 'Vehicle_Usage', 'Vehicle_Anti_Theft_Device', 'Vehicle_Passive_Restraint', 'Vehicle_Med_Pay_Limit', 'Vehicle_Bodily_Injury_Limit', 'Vehicle_Comprehensive_Coverage_Indicator', 'Vehicle_Comprehensive_Coverage_Limit', 'Vehicle_Collision_Coverage_Indicator', 'Vehicle_Collision_Coverage_Deductible', 'Vehicle_Youthful_Driver_Indicator', 'Vehicle_Youthful_Driver_Training_Code', 'Vehicle_Youthful_Good_Student_Code', 'Vehicle_Safe_Driver_Discount_Indicator', 'EEA_Liability_Coverage_Only_Indicator', 'EEA_Multi_Auto_Policies_Indicator', 'EEA_Policy_Zip_Code_3', 'EEA_Agency_Type', 'EEA_Packaged_Policy_Indicator', 'EEA_Full_Coverage_Indicator', 'EEA_Prior_Bodily_Injury_Limit', 'SYS_Renewed', 'SYS_New_Business']\n",
    "\n",
    "for col in claims_training_data.ix[1:]:\n",
    "    if col in categorical_columns:\n",
    "        claims_training_data[col] = claims_training_data[col].astype('category')\n",
    "\n",
    "cat_columns = claims_training_data.select_dtypes(['category']).columns\n",
    "\n",
    "claims_training_data[cat_columns] = claims_training_data[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "claims_training_data_with_claim = claims_training_data[claims_training_data.Claim_Count > 0].copy(deep=True)\n",
    "claims_training_data_without_claims = claims_training_data[claims_training_data.Claim_Count == 0]\n",
    "truncated_without_claims = claims_training_data_without_claims.head(n=8*claims_training_data_with_claim.shape[0])\n",
    "\n",
    "claims_training_data = claims_training_data_with_claim\n",
    "\n",
    "print(\"With claims\", claims_training_data_with_claim.shape)\n",
    "print(\"without\", claims_training_data_without_claims.shape)\n",
    "print(claims_training_data.shape)\n",
    "    \n",
    "def run_fit_reg(hiddenLayerssize, claims_training_data):\n",
    "    \n",
    "\n",
    "    X = claims_training_data.ix[:,0:-5]\n",
    "    losses = claims_training_data.ix[:,-4]\n",
    "    Y = losses#np.divide(losses, premium)\n",
    "    # scaler_x = StandardScaler()  \n",
    "    # print(X)\n",
    "    # X = pca.fit_transform(X)\n",
    "    # print(\"X after pca\", X)\n",
    "    # X = scaler_x.fit_transform(X)  \n",
    "\n",
    "    pipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=.95))])\n",
    "\n",
    "    X = pipeline.fit_transform(X)\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    mlp = MLPRegressor(max_iter=2000, hidden_layer_sizes=(hiddenLayerssize,hiddenLayerssize))#, hiddenLayerssize))\n",
    "\n",
    "    mlp.fit(X, Y)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return pipeline, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7864, 37)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# run both models\n",
    "pipeline, mlp = run_fit_reg(10, claims_training_data)\n",
    "\n",
    "\n",
    "#get classifications\n",
    "\n",
    "predicted_claims = []\n",
    "measureds = []\n",
    "for i in range(len(test_sets)):\n",
    "    X_t = test_sets[i].ix[:, 0:-5].copy(deep=True)\n",
    "    Y_t = test_sets[i].ix[:, -4].copy(deep=True)\n",
    "    for col in X_t:\n",
    "        X_t[col] = X_t[col].astype('category')\n",
    "    X_t = X_t.apply(lambda x: x.cat.codes)\n",
    "    Y_t = Y_t.apply(lambda x: 1 if x > 0 else 0)\n",
    "#     X_t = enc.transform(X_t)\n",
    "#     X_t = t_svd.transform(X_t)\n",
    "#     prediction = clf.predict(csr_matrix(X_t))\n",
    "    prediction = rfc.predict(fit.transform(X_t))\n",
    "    \n",
    "    predicted_claims.append(prediction)\n",
    "    measureds.append(Y_t.values)\n",
    "    \n",
    "# grab each record that will likely have a claim and predict    \n",
    "\n",
    "\n",
    "predictions = []\n",
    "measureds = []\n",
    "scores = []\n",
    "for i in range(len(test_sets)):\n",
    "    \n",
    "    X_t = test_sets[i].ix[:,0:-5].copy(deep=True)\n",
    "    Y_t = test_sets[i].ix[:,-4].copy(deep=True)\n",
    "    premium = sum(X_t.ix[:,-1])\n",
    "    measured = sum(Y_t)\n",
    "    \n",
    "    k = 0\n",
    "    for j in range(X_t.shape[0]):\n",
    "        if predicted_claims[i][j] == 0:\n",
    "            X_t.drop(X_t.index[k], inplace=True)\n",
    "            Y_t.drop(Y_t.index[k], inplace=True)\n",
    "        else:\n",
    "            k = k + 1\n",
    "            \n",
    "    \n",
    "    for col in X_t.ix[1:]:\n",
    "        if col in categorical_columns:\n",
    "            X_t[col] = X_t[col].astype('category')\n",
    "    X_t[cat_columns] = X_t[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    \n",
    "    X_t = pipeline.transform(X_t)\n",
    "    prediction = mlp.predict(X_t)\n",
    "    prediction = sum(prediction)\n",
    "    \n",
    "    predictions.append(prediction / premium)\n",
    "    measureds.append(measured / premium)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(predictions, 'r--')#np.divide(predictions, 10), 'r--')\n",
    "plt.plot(measureds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(measureds, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in 600 excel files lol \n",
    "predictions = []\n",
    "for i in range(1,601):\n",
    "    file_name = \"test_portfolios/test_portfolio_\"+str(i)+\".csv\"\n",
    "    TestData = read_data_file(file_name)\n",
    "#     print(TestData.shape)\n",
    "#     print(type(TestData))\n",
    "    TestData = TestData.drop('PolicyNo', 1)\n",
    "    X_t = TestData\n",
    "    premium = sum(X_t.ix[:,-1])\n",
    "    print(\"Premium:\", premium)\n",
    "    for col in X_t:\n",
    "        X_t[col] = X_t[col].astype('category')\n",
    "    X_t = X_t.apply(lambda x: x.cat.codes)\n",
    "    \n",
    "    # classificaztaion\n",
    "    \n",
    "    prediction = rfc.predict(fit.transform(X_t))\n",
    "    predicted_claims = prediction\n",
    "#     print (\"Predicted claims:\", predicted_claims)\n",
    "    #regression\n",
    "    X_t = TestData\n",
    "    k = 0\n",
    "    for j in predicted_claims:\n",
    "        if j == 0:\n",
    "            X_t.drop(X_t.index[k], inplace=True)\n",
    "        else:\n",
    "            k = k + 1\n",
    "            \n",
    "    for col in X_t:\n",
    "        if col in categorical_columns:\n",
    "            X_t[col] = X_t[col].astype('category')\n",
    "    X_t[cat_columns] = X_t[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    \n",
    "    X_t = pipeline.transform(X_t)\n",
    "    prediction = mlp.predict(X_t)\n",
    "    prediction = sum(prediction)\n",
    "    \n",
    "    print(\"Sum Predicitpon\", prediction)\n",
    "    predictions.append(prediction / premium)\n",
    "    \n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
